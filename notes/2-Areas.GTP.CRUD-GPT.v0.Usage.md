---
id: 63swl3rjq5mhbddr4i0u556
title: Usage
desc: ''
updated: 1742536306213
created: 1742536306213
---
## Usage Workflow: Step-by-Step Guide

Using the multi-GPT system involves a series of well-defined steps. Below is a high-level guide to building a project with this architecture, along with tips for troubleshooting:

**1. Initial Setup – Configure GPT Agents:** Begin by creating the custom GPTs in your ChatGPT desktop app. For each role in the architecture (Prompters, Workers, QA for each workgroup), set up a new custom GPT with the provided instructions and context. For example, create a “Frontend Page Prompter” GPT and paste in its persona instructions (from our documentation) which tell it how to output structured page requirements. Do this for all needed agents. Organize them (ChatGPT Desktop allows grouping or at least you can name each chat) by their team/workgroup for clarity. This one-time setup establishes your AI work team.

**2. Input Project Requirements:** Start a new conversation with the top-level planner (or you can directly use a specific team’s Prompter if you know which part to tackle first). Input the PRD, user story, or feature specification you want to implement. For example, if the feature is “User Login”, feed the relevant excerpt of the PRD to the Frontend Page Prompter GPT (since this will involve creating a login page and possibly an API). The Prompter will ask any clarifying questions if needed (e.g., “Does the login require 2FA?”). Answer these as necessary. Once the Prompter GPT has all info, it will output a structured plan for that part of the system.

**3. Trigger the Worker GPT:** Take the structured prompt from the Prompter and send it to the corresponding Worker GPT. In our example, the Frontend Page Worker GPT receives the detailed instructions about the Login page UI and functionality. It will then generate the code for the Next.js page component. The output might include a code block for `app/login/page.tsx`, possibly additional components or hooks if the prompt indicated those. Copy all these outputs to a text editor or your project files. (If the output is long, ensure it wasn’t cut off. If it was, you can prompt the Worker GPT with “continue” or break the input into smaller parts and generate sequentially).

**4. Run QA Checks:** Now take the Worker’s output and present it to the QA Checker GPT for that workgroup. For the Login page example, feed the code to the Frontend QA GPT, possibly with a prompt like “Review the above code for any issues or improvements.” The QA GPT will analyze it against its checklist. If everything looks good, it might respond with “All checks passed. The code meets the requirements.” More likely, it will flag at least minor issues or suggestions (e.g., “The password input should have `type=\"password\"` for security​,” or “Consider adding a loading spinner on form submit for better UX”). If issues are found, address them: you can either manually edit the code as per QA advice or loop back to the Worker GPT. For instance, tell the Frontend Worker GPT, “Incorporate a loading state as per QA feedback,” and it will adjust the code. Re-run the QA GPT on the revised code until it approves. This iterative loop ensures quality before moving on.

**5. Iterate for All Workgroups:** Continue the above process for other parts of the feature. In the login example, after the page, you’d involve the **Auth and API workgroups** from the Backend (Lib Team). So you go to the API Prompter GPT with something like “We need an API endpoint to authenticate user login. Here are the requirements…” The API Prompter will structure input for the API Worker GPT, which then produces code (e.g., `app/api/login/route.ts` in Next.js 15). Then send that to the Backend QA GPT to verify security (ensuring it checks password hashing, correct status codes, etc.). If the project also requires a database model (a User model), use the Models Prompter and Worker similarly. In short, for each aspect – frontend UI, backend route, database – engage the respective GPT pipeline. Thanks to the hierarchical design, each GPT knows what context to expect. The **Type System Team** (Type Checker) might be invoked after integrating frontend and backend outputs: you could prompt it with “Ensure all types align between frontend and backend for the login feature,” and it would output any type/interface definitions or corrections needed. By iterating through all relevant workgroups, you assemble the full implementation of the feature.

**6. Integrate Outputs into Project:** As you obtain approved outputs, integrate them into your local project folder. Create the files as named and paste the code. Since the GPTs followed the project structure, this should be straightforward. For example, put the generated `page.tsx` under `app/login/`, the `route.ts` under `app/api/login/`, etc., as instructed by the file paths in GPT outputs. Add any new component files to the `components/` directory. It’s wise to use version control locally – check in these new files with a descriptive commit message (the GPT system can even generate a suggested commit message if you ask one of the GPTs acting as a project manager).

**7. Test Locally:** Run the project locally (e.g., `npm run dev` for Next.js). Manually test the new feature. If you encounter errors or the app crashes, use the error messages as feedback. You can actually feed error logs back to a GPT (perhaps the QA GPT or even the original Worker GPT) and ask for help. For instance, if launching the app yields a TypeScript compilation error, copy the error text and ask the relevant GPT, “How do I fix this?” Often, the Type System GPT or the specific Worker that wrote that code can pinpoint the mistake (maybe a variable undefined or a type mismatch) and provide a correction. This is a manual debugging assist. Ensure the feature works as expected (log in with a test user, see if the network request goes through). Any logical bugs (like incorrect login validation) can be iteratively fixed by describing the problem to the GPT and letting it suggest code changes.

**8. Documentation & Final Touches:** Optionally, use the GPT system to generate documentation for the feature. For example, you might have a “Docs Writer” GPT (not explicitly defined earlier, but you can use one of the existing GPTs or the Prompter with a different instruction) to produce a README section or update the API docs for the new endpoint. This ensures your project docs stay up-to-date. The GPT could output Markdown describing the login feature, which you then add to your documentation.

**9. Deploy to Vercel:** Once satisfied locally, it’s time to deploy. Push your changes to GitHub (e.g., `git push origin main`). If you have Vercel connected, it will automatically build and deploy the project. Our CI/CD GPTs would have set up things so that this process is smooth – for example, ensuring the `vercel.json` specifies the correct output directory or that environment variables are defined. Vercel’s GitHub integration provides preview URLs and will promote to production on merging, etc., as configured​. Monitor the deployment logs on Vercel; they should pass if our QA steps were thorough. If a build fails on Vercel (perhaps due to something like a case-sensitive path issue or a missing dependency), take that log and consult the relevant GPT or fix manually, then redeploy. After deployment, test the feature on the live URL.

**10. Maintenance Workflow:** As you continue development, follow the same procedure for each new feature or change. Treat each GPT workgroup as you would a member of your dev team: involve them in planning (Prompters), execution (Workers), and review (QA). If requirements change, update the relevant inputs and maybe re-run a Prompter to get an updated plan. Always keep the documentation updated via GPT or manual notes so that the next person (or GPT) working on the project has an accurate picture. Over time, you might find ways to streamline this; for example, you might script some of the copy-paste steps or use ChatGPT’s **Projects** feature (when it matures) to have a single session manage multiple files.

**Troubleshooting Tips:** Because this is a manual orchestration, here are a few common issues and solutions:

- If a GPT’s output is not what you expected (too verbose, or not following format), double-check that you gave it the correct instructions and context. You may need to remind the Worker of the format by showing a brief example in the prompt. Consistency in how you prompt them will yield consistent results.
- Sometimes a QA GPT might be overly pedantic or “hallucinate” an issue that isn’t real. Use your judgment; you don’t have to implement every suggestion if you know it’s optional or stylistic. The GPTs are assistants, not ultimate authorities. You can also ask the QA GPT to clarify a recommendation if it’s unclear.
- If two workgroups produce conflicting outputs (e.g., the Frontend expects an API response in a certain shape but the Backend returned a different shape), this is where the Type System team or your own review must resolve it. In such cases, you can either adjust one side manually or prompt one of the GPTs with the other’s output to reconcile. For instance, show the Backend Worker what the Frontend expects and have it adjust the API accordingly.
- Manage the state between chats diligently. Since there’s no automatic memory sharing between these custom GPTs (by design, no API calls), **you** are the conduit. Keep notes of what each GPT has produced. It helps to maintain a simple log or use the project’s README to accumulate the decisions (almost like meeting minutes of an engineering sprint). This will prevent confusion if, say, you come back the next day and need to recall where you left off.

By following these steps, you can develop features systematically. After a few cycles, you’ll find the rhythm and the “AI workgroup” becomes a natural extension of your development process. The result is a collaboratively built project where AI handled much of the heavy lifting under your guidance. Each phase – design, coding, testing, deployment – is covered by the appropriate GPT, and you retain full control without needing direct API integration.

[medium.com](https://medium.com/@pallavisinha12/understanding-llm-based-agents-and-their-multi-agent-architecture-299cf54ebae4#:~:text=Send%20personalized%20tips%20as%20an,colored%20clothes%2C%20etc)
[vercel.com](https://vercel.com/docs/git/vercel-for-github#:~:text=Deploying%20GitHub%20Projects%20with%20Vercel,and%20automatic%20Custom%20Domain%20updates)

